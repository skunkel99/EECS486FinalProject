{
    "paper_id": "0a0af20f552c58dd3d3b0f9d937d7648ba59f7fc",
    "metadata": {
        "title": "Participation in TREC 2020 COVID Track Using Continuous Active Learning",
        "authors": [
            {
                "first": "Jean",
                "middle": [],
                "last": "Xue",
                "suffix": "",
                "affiliation": {
                    "laboratory": "",
                    "institution": "University of Waterloo Waterloo",
                    "location": {
                        "region": "ON",
                        "country": "Canada"
                    }
                },
                "email": ""
            },
            {
                "first": "Jun",
                "middle": [],
                "last": "Wang",
                "suffix": "",
                "affiliation": {
                    "laboratory": "",
                    "institution": "University of Waterloo Waterloo",
                    "location": {
                        "region": "ON",
                        "country": "Canada"
                    }
                },
                "email": "xj4wang@uwaterloo.ca"
            },
            {
                "first": "Maura",
                "middle": [
                    "R"
                ],
                "last": "Grossman",
                "suffix": "",
                "affiliation": {
                    "laboratory": "",
                    "institution": "University of Waterloo Waterloo",
                    "location": {
                        "region": "ON",
                        "country": "Canada"
                    }
                },
                "email": "maura.grossman@uwaterloo.ca"
            },
            {
                "first": "Kevin",
                "middle": [],
                "last": "",
                "suffix": "",
                "affiliation": {
                    "laboratory": "",
                    "institution": "University of Waterloo Waterloo",
                    "location": {
                        "region": "ON",
                        "country": "Canada"
                    }
                },
                "email": ""
            },
            {
                "first": "Seung",
                "middle": [],
                "last": "Gyu",
                "suffix": "",
                "affiliation": {
                    "laboratory": "",
                    "institution": "University of Waterloo Waterloo",
                    "location": {
                        "region": "ON",
                        "country": "Canada"
                    }
                },
                "email": ""
            },
            {
                "first": "Hyun",
                "middle": [],
                "last": "",
                "suffix": "",
                "affiliation": {
                    "laboratory": "",
                    "institution": "University of Waterloo Waterloo",
                    "location": {
                        "region": "ON",
                        "country": "Canada"
                    }
                },
                "email": "sghyun@uwaterloo.ca"
            }
        ]
    },
    "abstract": [
        {
            "text": "We describe our participation in all five rounds of the TREC 2020 COVID Track (TREC-COVID). The goal of TREC-COVID is to contribute to the response to the COVID-19 pandemic by identifying answers to many pressing questions and building infrastructure to improve search systems [8] . All five rounds of this Track challenged participants to perform a classic ad-hoc search task on the new data collection CORD-19. Our solution addressed this challenge by applying the Continuous Active Learning model (CAL) and its variations. Our results showed us to be amongst the top scoring manual runs and we remained competitive within all categories of submissions.",
            "cite_spans": [
                {
                    "start": 277,
                    "end": 280,
                    "text": "[8]",
                    "ref_id": "BIBREF7"
                }
            ],
            "ref_spans": [],
            "section": "Abstract"
        }
    ],
    "body_text": [
        {
            "text": "As the spread of COVID-19 continues around the globe, researchers, clinicians, and policy makers involved with its response are constantly searching for reliable information on the virus. This presents those of us in information retrieval (IR) and text processing communities with a unique opportunity to contribute to the response to this pandemic by building infrastructure to improve search systems and to help identify answers for some of today's most pressing questions [8] . The task of TREC-COVID is for participants to retrieve the most relevant documents from the CORD-19 data-set for a given set of topics. To address this challenge, we implemented a system based on CAL, following the work of Grossman and Cormack in [3, 4] , using the tool kit provided as part of the Baseline Model Implementation (BMI), created by Roegiest and Cormack in [6] , and ourselves as the human assessors.",
            "cite_spans": [
                {
                    "start": 475,
                    "end": 478,
                    "text": "[8]",
                    "ref_id": "BIBREF7"
                },
                {
                    "start": 728,
                    "end": 731,
                    "text": "[3,",
                    "ref_id": "BIBREF2"
                },
                {
                    "start": 732,
                    "end": 734,
                    "text": "4]",
                    "ref_id": "BIBREF3"
                },
                {
                    "start": 852,
                    "end": 855,
                    "text": "[6]",
                    "ref_id": "BIBREF5"
                }
            ],
            "ref_spans": [],
            "section": "Introduction"
        },
        {
            "text": "In this section, we discuss prior research on CAL. We then discuss prior research on BMI, which provides the tool kits we heavily relied upon for this challenge.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Related Work"
        },
        {
            "text": "Continuous Active Learning (CAL). CAL is a method for finding virtually all relevant information on a particular subject within a vast sea of electronically stored information (ESI): it repeatedly refines its understanding about which of the remaining documents are most likely to be of interest, based on the users' feedback regarding the documents already judged [4] . This protocol is most famously used in technologyassisted review (TAR) for electronic discovery in legal matters, achieving the best results reported in scientific literature to date [2] . Building on the CAL protocol, many implementations, such as BMI, have been highly successful at performing ad-hoc retrieval tasks, such as in the TREC 2015/2016 Total Recall Track [7, 5] and the TREC 2019 Decision Track [1] .",
            "cite_spans": [
                {
                    "start": 365,
                    "end": 368,
                    "text": "[4]",
                    "ref_id": "BIBREF3"
                },
                {
                    "start": 554,
                    "end": 557,
                    "text": "[2]",
                    "ref_id": "BIBREF1"
                },
                {
                    "start": 740,
                    "end": 743,
                    "text": "[7,",
                    "ref_id": "BIBREF6"
                },
                {
                    "start": 744,
                    "end": 746,
                    "text": "5]",
                    "ref_id": "BIBREF4"
                },
                {
                    "start": 780,
                    "end": 783,
                    "text": "[1]",
                    "ref_id": "BIBREF0"
                }
            ],
            "ref_spans": [],
            "section": "Related Work"
        },
        {
            "text": "Baseline Model Implementation (BMI). BMI is an augmented version of CAL. It is autonomous and was initially made available to participants of the TREC 2015/2016 Total Recall Tracks [7, 5] , as well as the TREC 2019 Decision Track [1] to provide a baseline for comparison. However, BMI turned out to be highly competitive, with none of the manual participants achieving consistently superior results to this fully automated method [6] .",
            "cite_spans": [
                {
                    "start": 181,
                    "end": 184,
                    "text": "[7,",
                    "ref_id": "BIBREF6"
                },
                {
                    "start": 185,
                    "end": 187,
                    "text": "5]",
                    "ref_id": "BIBREF4"
                },
                {
                    "start": 230,
                    "end": 233,
                    "text": "[1]",
                    "ref_id": "BIBREF0"
                },
                {
                    "start": 430,
                    "end": 433,
                    "text": "[6]",
                    "ref_id": "BIBREF5"
                }
            ],
            "ref_spans": [],
            "section": "Related Work"
        },
        {
            "text": "While BMI has been shown to generally outperform human-in-the-loop CAL implementations [6] , it requires labelled data, which was very limited, if available at all, for TREC-COVID; thus, we chose to insert a human back into the loop to make judgements. All other components, such as creating feature vectors, the learner, etc., were taken directly from the BMI tool kits.",
            "cite_spans": [
                {
                    "start": 87,
                    "end": 90,
                    "text": "[6]",
                    "ref_id": "BIBREF5"
                }
            ],
            "ref_spans": [],
            "section": "Related Work"
        },
        {
            "text": "Document Set Processing. The document set used in the TREC-COVID Challenge is the COVID-19 Open Research Data-set (CORD-19). Our team opted to judge a document's relevancy using strictly the information available in the metadata file (year, authors, publisher, title, abstract) based on the work of Zhang et al. [9] which show that participants achieve higher recall using CAL when presented with only a single short excerpt rather than an entire document.",
            "cite_spans": [
                {
                    "start": 312,
                    "end": 315,
                    "text": "[9]",
                    "ref_id": "BIBREF8"
                }
            ],
            "ref_spans": [],
            "section": "System Overview & General Approach"
        },
        {
            "text": "CAL. The following shows an outline of our specific implementation of CAL.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "System Overview & General Approach"
        },
        {
            "text": "\u2022 STEP 1: Create a hypothetical relevant document, known as a synthetic document.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "System Overview & General Approach"
        },
        {
            "text": "To create the synthetic documents, we concatenated the query, question, and narrative components of the topics file provided by TREC-COVID, as shown in Figures 1 and 2 . \u2022 STEP 2: Use a machine-learning algorithm to suggest the next most-likely relevant document.",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 152,
                    "end": 167,
                    "text": "Figures 1 and 2",
                    "ref_id": "FIGREF0"
                }
            ],
            "section": "System Overview & General Approach"
        },
        {
            "text": "The machine-learning algorithm we chose is Sofia-ML which Roegiest and Cormack used in their participation in the 2015 Total Recall Track [6] .",
            "cite_spans": [
                {
                    "start": 138,
                    "end": 141,
                    "text": "[6]",
                    "ref_id": "BIBREF5"
                }
            ],
            "ref_spans": [],
            "section": "System Overview & General Approach"
        },
        {
            "text": "\u2022 STEP 3: Review the suggested documents and provide relevance feedback to the learning algorithm, indicating whether each suggested document is actually relevant or not.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "System Overview & General Approach"
        },
        {
            "text": "To do this, we sorted the results given by Sofia-ML in decreasing order of confidence, presenting the top most result to the human assessor using a text based user interface. The judgement made by our human assessors is one of {0-not relevant, 1-partially relevant, 2-relevant}. This corresponds to the annotations made by biomedical experts as part of TREC-COVID following each round. As Sofia-ML does not distinguish between relevant judgements and partially relevant judgments, both were designated to be relevant in training.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "System Overview & General Approach"
        },
        {
            "text": "\u2022 STEP 4: Repeat Step 2 and 3 until very few, if any, of the suggested documents are relevant.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "System Overview & General Approach"
        },
        {
            "text": "Using the same stopping condition as in [5] , we aimed to stop when the following criterion was met:",
            "cite_spans": [
                {
                    "start": 40,
                    "end": 43,
                    "text": "[5]",
                    "ref_id": "BIBREF4"
                }
            ],
            "ref_spans": [],
            "section": "System Overview & General Approach"
        },
        {
            "text": "where m is the number of relevant documents reviewed, n is the number of irrelevant documents reviewed, a is a constant which determines how many non-relevant documents are to be reviewed in the course of finding each relevant document, and b is a constant which represents a fixed overhead for the number of irrelevant documents that must be reviewed.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "System Overview & General Approach"
        },
        {
            "text": "S-CAL. One of the major drawbacks of the CAL method outlined above is the impractical number of documents that must be reviewed when the number of relevant documents is large. Scalable Continuous Active Learning (S-CAL) [3] addresses this issue by 1. Segmenting the corpus into batches and allowing assessors to label only a small finite sample of documents from each successive batch.",
            "cite_spans": [
                {
                    "start": 220,
                    "end": 223,
                    "text": "[3]",
                    "ref_id": "BIBREF2"
                }
            ],
            "ref_spans": [],
            "section": "System Overview & General Approach"
        },
        {
            "text": "2. Temporarily augmenting each training set by adding a set of 100 random documents from the corpus -which is, with high probability, not relevant for a large corpus -labelled not relevant.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "System Overview & General Approach"
        },
        {
            "text": "However, the stopping condition for S-CAL outlined in [3] is still infeasible to achieve with CORD-19 and our team size; thus, we exchange the initial dynamic stopping condition for a static goal of assessing 300 documents per topic.",
            "cite_spans": [
                {
                    "start": 54,
                    "end": 57,
                    "text": "[3]",
                    "ref_id": "BIBREF2"
                }
            ],
            "ref_spans": [],
            "section": "System Overview & General Approach"
        },
        {
            "text": "Hyper-parameter Tuning. Given the availability of labelled data after the first round, we performed hyper-parameter tuning on both the loop type and the lambda value to better fit CORD-19. Finding no significant differences in our tests, we decided to continue with our initial values taken from [6] , which were decided upon discussion with the author of Sofia-ML as well as their internal experiments.",
            "cite_spans": [
                {
                    "start": 296,
                    "end": 299,
                    "text": "[6]",
                    "ref_id": "BIBREF5"
                }
            ],
            "ref_spans": [],
            "section": "System Overview & General Approach"
        },
        {
            "text": "Creating Runs. To generate the results for our runs, we created lists of 1000 documents ordered as shown in Figure 3 . Key-Term Highlighting. Key-term highlighting is a feature commonly provided by IR systems, such as Google, to assist human readers in processing information. Following the online sample of CAL, as show in Figure 4 , given as a supplement to [4] , we chose to highlight the top five highest-scoring words from a document, according to Sofia-ML, in our UI for assessors, as show in Figure 5 . Table 1 shows the specifications of our system for each round of TREC-COVID and Table 2 shows our results. From these, we are able to make some interesting observations:",
            "cite_spans": [
                {
                    "start": 360,
                    "end": 363,
                    "text": "[4]",
                    "ref_id": "BIBREF3"
                }
            ],
            "ref_spans": [
                {
                    "start": 108,
                    "end": 116,
                    "text": "Figure 3",
                    "ref_id": "FIGREF2"
                },
                {
                    "start": 324,
                    "end": 332,
                    "text": "Figure 4",
                    "ref_id": "FIGREF3"
                },
                {
                    "start": 499,
                    "end": 507,
                    "text": "Figure 5",
                    "ref_id": "FIGREF4"
                },
                {
                    "start": 510,
                    "end": 517,
                    "text": "Table 1",
                    "ref_id": "TABREF0"
                },
                {
                    "start": 590,
                    "end": 597,
                    "text": "Table 2",
                    "ref_id": "TABREF1"
                }
            ],
            "section": "System Overview & General Approach"
        },
        {
            "text": "1. Despite our human assessors having provided more labelled documents in round 2 than round 1, our performance decreased. One possible explanation could be that, through the use of the key-term highlighting feature, our human assessor(s) exchanged quantity for quality resulting in an overall poorer model.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Results and Discussion"
        },
        {
            "text": "Despite being able to provide more labelled documents in round 5 than 4, our performance once again decreased. One possible explanation could be that we did not perform the necessary quality control required for additional human assessors -once again, exchanging quantity for quality labels, resulting in an overall poorer performance.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "2."
        },
        {
            "text": "3. The runs ordered by method Figure 3 (iii) consistently outperformed our other runs. This could imply that the documents judged to be not-relevant by our assessors are still more relevant than Sofia-ML's labelling of unseen documents.",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 30,
                    "end": 38,
                    "text": "Figure 3",
                    "ref_id": "FIGREF2"
                }
            ],
            "section": "2."
        },
        {
            "text": "In this paper, we report on our participation to the TREC 2020 COVID Track rounds 1 though 5, describing our approach, results, and lessons learned. We initially use CAL [4] , implemented using tools from BMI's feature kit [6] , with ourselves as the annotators. The large human labelling effort required for our system motivated us to implement a key-term highlighting feature, use S-CAL [3] , and recruit more human assessors. The results in Table 3 show us to be among the top-scoring manual runs and competitive within all categories of submissions throughout all rounds. Our results in Table 2 also bring up an age-old question of quantity versus quality when it comes to data in IR.",
            "cite_spans": [
                {
                    "start": 170,
                    "end": 173,
                    "text": "[4]",
                    "ref_id": "BIBREF3"
                },
                {
                    "start": 223,
                    "end": 226,
                    "text": "[6]",
                    "ref_id": "BIBREF5"
                },
                {
                    "start": 389,
                    "end": 392,
                    "text": "[3]",
                    "ref_id": "BIBREF2"
                }
            ],
            "ref_spans": [
                {
                    "start": 444,
                    "end": 451,
                    "text": "Table 3",
                    "ref_id": "TABREF2"
                },
                {
                    "start": 591,
                    "end": 598,
                    "text": "Table 2",
                    "ref_id": "TABREF1"
                }
            ],
            "section": "Conclusion"
        },
        {
            "text": "Document set processing, CAL, 1 assessor xj4wang run1: ordered by method (i)",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Round 1"
        },
        {
            "text": "Being pressed for time, we were unable to reach our stopping condition, prematurely stopping after 40 document assessments for each topic.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Round 1"
        },
        {
            "text": "Using sort -rn instead of sort -rg resulting in documents with exponentially low confidence being sorted to the top during both the assessing process and the run creation.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Round 1"
        },
        {
            "text": "Same as Round 1, + Key-term highlighting xj4wang run3: ordered by method (i)",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Round 2"
        },
        {
            "text": "Being pressed for time, we were unable to reach our stopping condition, prematurely stopping after 60 document assessments for each topic.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Round 2"
        },
        {
            "text": "Same as Round 2, \u00b1 Switching out CAL for S-CAL, + 1 additional assessor, total of 2 xj4wang run1: ordered by method (iii) xj4wang run2: ordered by method (ii) xj4wang run3: ordered by method (i)",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Round 3"
        },
        {
            "text": "Being pressed for time, we were unable to reach our stopping condition for every topic.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Round 3"
        },
        {
            "text": "Same as Round 3, + 1 additional assessor, total of 3 Same as Round 3",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Round 4"
        },
        {
            "text": "Same as Round 4, + 2 additional assessor, total of 5 Same as Round 3 ",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Round 5"
        }
    ],
    "bib_entries": {
        "BIBREF0": {
            "ref_id": "b0",
            "title": "Uwaterloomds at the trec 2019 decision track",
            "authors": [
                {
                    "first": "Mustafa",
                    "middle": [],
                    "last": "Abualsaud",
                    "suffix": ""
                },
                {
                    "first": "C",
                    "middle": [],
                    "last": "Fuat",
                    "suffix": ""
                },
                {
                    "first": "Mark",
                    "middle": [
                        "D"
                    ],
                    "last": "Beylunioglu",
                    "suffix": ""
                },
                {
                    "first": "P",
                    "middle": [
                        "Robert"
                    ],
                    "last": "Smucker",
                    "suffix": ""
                },
                {
                    "first": "",
                    "middle": [],
                    "last": "Duimering",
                    "suffix": ""
                }
            ],
            "year": 2019,
            "venue": "NIST Special Publication 1250: The Twenty-Eighth Text REtrieval Conference Proceedings (TREC 2019)",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF1": {
            "ref_id": "b1",
            "title": "Evaluation of machine-learning protocols for technologyassisted review in electronic discovery",
            "authors": [
                {
                    "first": "V",
                    "middle": [],
                    "last": "Gordon",
                    "suffix": ""
                },
                {
                    "first": "Maura",
                    "middle": [
                        "R"
                    ],
                    "last": "Cormack",
                    "suffix": ""
                },
                {
                    "first": "",
                    "middle": [],
                    "last": "Grossman",
                    "suffix": ""
                }
            ],
            "year": 2014,
            "venue": "Proceedings of the 37th International ACM SIGIR Conference on Research & Development in Information Retrieval, SIGIR '14",
            "volume": "",
            "issn": "",
            "pages": "153--162",
            "other_ids": {}
        },
        "BIBREF2": {
            "ref_id": "b2",
            "title": "Scalability of continuous active learning for reliable highrecall text classification",
            "authors": [
                {
                    "first": "V",
                    "middle": [],
                    "last": "Gordon",
                    "suffix": ""
                },
                {
                    "first": "Maura",
                    "middle": [
                        "R"
                    ],
                    "last": "Cormack",
                    "suffix": ""
                },
                {
                    "first": "",
                    "middle": [],
                    "last": "Grossman",
                    "suffix": ""
                }
            ],
            "year": 2016,
            "venue": "Proceedings of the 25th ACM international on conference on information and knowledge management",
            "volume": "",
            "issn": "",
            "pages": "1039--1048",
            "other_ids": {}
        },
        "BIBREF3": {
            "ref_id": "b3",
            "title": "Continuous active learning for tar",
            "authors": [
                {
                    "first": "Maura",
                    "middle": [
                        "R"
                    ],
                    "last": "Grossman",
                    "suffix": ""
                },
                {
                    "first": "Gordon",
                    "middle": [
                        "V"
                    ],
                    "last": "Cormack",
                    "suffix": ""
                }
            ],
            "year": 2016,
            "venue": "Practical Law Journal",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF4": {
            "ref_id": "b4",
            "title": "Trec 2016 total recall track overview",
            "authors": [
                {
                    "first": "Gordon",
                    "middle": [
                        "V"
                    ],
                    "last": "Maura R Grossman",
                    "suffix": ""
                },
                {
                    "first": "Adam",
                    "middle": [],
                    "last": "Cormack",
                    "suffix": ""
                },
                {
                    "first": "",
                    "middle": [],
                    "last": "Roegiest",
                    "suffix": ""
                }
            ],
            "year": 2016,
            "venue": "NIST Special Publication 500-319: The Twenty-Fourth Text REtrieval Conference Proceedings (TREC 2015)",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF5": {
            "ref_id": "b5",
            "title": "Total recall track tools architecture overview",
            "authors": [
                {
                    "first": "Adam",
                    "middle": [],
                    "last": "Roegiest",
                    "suffix": ""
                },
                {
                    "first": "V",
                    "middle": [],
                    "last": "Gordon",
                    "suffix": ""
                },
                {
                    "first": "",
                    "middle": [],
                    "last": "Cormack",
                    "suffix": ""
                }
            ],
            "year": 2015,
            "venue": "Proc. TREC-2015",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF6": {
            "ref_id": "b6",
            "title": "Trec 2015 total recall track overview",
            "authors": [
                {
                    "first": "Adam",
                    "middle": [],
                    "last": "Roegiest",
                    "suffix": ""
                },
                {
                    "first": "V",
                    "middle": [],
                    "last": "Gordon",
                    "suffix": ""
                },
                {
                    "first": "",
                    "middle": [],
                    "last": "Cormack",
                    "suffix": ""
                },
                {
                    "first": "L",
                    "middle": [
                        "A"
                    ],
                    "last": "Charles",
                    "suffix": ""
                },
                {
                    "first": "Maura",
                    "middle": [
                        "R"
                    ],
                    "last": "Clarke",
                    "suffix": ""
                },
                {
                    "first": "",
                    "middle": [],
                    "last": "Grossman",
                    "suffix": ""
                }
            ],
            "year": 2015,
            "venue": "NIST Special Publication 500-319: The Twenty-Fourth Text REtrieval Conference Proceedings (TREC 2015)",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF7": {
            "ref_id": "b7",
            "title": "Trec-covid: Constructing a pandemic information retrieval test collection",
            "authors": [
                {
                    "first": "Ellen",
                    "middle": [],
                    "last": "Voorhees",
                    "suffix": ""
                },
                {
                    "first": "Tasmeer",
                    "middle": [],
                    "last": "Alam",
                    "suffix": ""
                },
                {
                    "first": "Steven",
                    "middle": [],
                    "last": "Bedrick",
                    "suffix": ""
                },
                {
                    "first": "Dina",
                    "middle": [],
                    "last": "Demner-Fushman",
                    "suffix": ""
                },
                {
                    "first": "R",
                    "middle": [],
                    "last": "William",
                    "suffix": ""
                },
                {
                    "first": "Kyle",
                    "middle": [],
                    "last": "Hersh",
                    "suffix": ""
                },
                {
                    "first": "Kirk",
                    "middle": [],
                    "last": "Lo",
                    "suffix": ""
                },
                {
                    "first": "Ian",
                    "middle": [],
                    "last": "Roberts",
                    "suffix": ""
                },
                {
                    "first": "Lucy",
                    "middle": [
                        "Lu"
                    ],
                    "last": "Soboroff",
                    "suffix": ""
                },
                {
                    "first": "",
                    "middle": [],
                    "last": "Wang",
                    "suffix": ""
                }
            ],
            "year": 2020,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {
                "arXiv": [
                    "arXiv:2005.04474"
                ]
            }
        },
        "BIBREF8": {
            "ref_id": "b8",
            "title": "Effective user interaction for high-recall retrieval: Less is more",
            "authors": [
                {
                    "first": "Haotian",
                    "middle": [],
                    "last": "Zhang",
                    "suffix": ""
                },
                {
                    "first": "Mustafa",
                    "middle": [],
                    "last": "Abualsaud",
                    "suffix": ""
                },
                {
                    "first": "Nimesh",
                    "middle": [],
                    "last": "Ghelani",
                    "suffix": ""
                },
                {
                    "first": "Mark",
                    "middle": [
                        "D"
                    ],
                    "last": "Smucker",
                    "suffix": ""
                },
                {
                    "first": "Gordon",
                    "middle": [
                        "V"
                    ],
                    "last": "Cormack",
                    "suffix": ""
                },
                {
                    "first": "Maura",
                    "middle": [
                        "R"
                    ],
                    "last": "Grossman",
                    "suffix": ""
                }
            ],
            "year": 2018,
            "venue": "Proceedings of the 27th ACM International Conference on Information and Knowledge Management, CIKM '18",
            "volume": "",
            "issn": "",
            "pages": "187--196",
            "other_ids": {}
        }
    },
    "ref_entries": {
        "FIGREF0": {
            "text": "The synthetic document for topic 1.",
            "latex": null,
            "type": "figure"
        },
        "FIGREF1": {
            "text": "Snippet of topic 1 in the xml topics file provided by TREC-COVID.",
            "latex": null,
            "type": "figure"
        },
        "FIGREF2": {
            "text": "Document orderings for runs. (i) Lead with documents labelled relevant, followed by partially relevant, and finally filled by Sofia-ML's labelling of unseen documents in descending order of confidence. (ii) All documents are arranged using Sofia-ML. No special consideration is given to documents already assessed by a human assessor. (iii) Keep documents that annotators have labelled to be not relevant in the final run.",
            "latex": null,
            "type": "figure"
        },
        "FIGREF3": {
            "text": "A sample document retrieved from Grossman and Cormack's online CAL platform, https://cormack.uwaterloo.ca/cal/, showing key-term highlighting.",
            "latex": null,
            "type": "figure"
        },
        "FIGREF4": {
            "text": "Text based user interface showing keyterm highlighting.",
            "latex": null,
            "type": "figure"
        },
        "TABREF0": {
            "text": "Specifications of system design for each round of TREC-COVID.",
            "latex": null,
            "type": "table"
        },
        "TABREF1": {
            "text": "Results of all runs of team xj4wang.",
            "latex": null,
            "type": "table"
        },
        "TABREF2": {
            "text": "Results of team xj4wang and the maximum scores obtained per measurement across all different teams.",
            "latex": null,
            "type": "table"
        }
    },
    "back_matter": [
        {
            "text": "A special thanks goes to Gordon Cormack for his valuable guidance and Anmol Singh for his insights. We would also like to thank Charlotte Stinson, Eric Sheen, and Solaiappan Alagappan for the time and effort they spent assessing these documents. ",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Acknowledgement"
        }
    ]
}